{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2609d0b8-e033-455e-8554-48b714668531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PROGETTO BIG DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccee5af-5efd-4274-8e7a-dd9ddd615b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Importazione librerie e download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ca38be05-5d7b-40bf-bfb2-a131b400e220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install wordcloud nltk pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e1360243-1356-4a4b-bc30-e79d2bc71307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "apt-get update && apt-get install -y python3-pil.imagetk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "115a09ca-4463-4c56-9d92-2a2515ea1883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d8a1150b-0472-4148-b614-0d4f053e7393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "189e9cd7-5178-40a4-83bc-c81673af0987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install wordcloud==1.9.2 nltk==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6365b404-b390-4c71-9d10-8069bcdca34b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "apt-get update\n",
    "apt-get install -y fonts-dejavu\n",
    "fc-cache -f -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1574c56b-cd12-4b10-b89a-ff151f6cdf78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Verifica la directory corrente\n",
    "print(\"Directory corrente:\", os.getcwd())\n",
    "# Verifica spazio disponibile\n",
    "import shutil\n",
    "total, used, free = shutil.disk_usage(\"/\")\n",
    "print(f\"Spazio libero: {free // (2**30)} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c622aff-04c4-4e02-8a11-b95de39bf5c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from typing import Tuple, Optional, List, Dict\n",
    "import string  # Per string.punctuation\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f732c674-eaf9-4b68-bb90-5cf606af563e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset caricato con successo!\n",
      "Shape del dataset: (153232, 5)\n"
     ]
    }
   ],
   "source": [
    "# Funzione per il download del dataset\n",
    "def download_in_chunks(url, chunk_size=1024):\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        chunks = []\n",
    "        for chunk in response.iter_content(chunk_size=chunk_size, decode_unicode=True):\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        return ''.join(chunks)\n",
    "    return None\n",
    "\n",
    "# URL del dataset\n",
    "url = \"https://proai-datasets.s3.eu-west-3.amazonaws.com/wikipedia.csv\"\n",
    "\n",
    "# Download e caricamento dati\n",
    "try:\n",
    "    data = download_in_chunks(url)\n",
    "    if data:\n",
    "        df = pd.read_csv(StringIO(data))\n",
    "        print(\"Dataset caricato con successo!\")\n",
    "        print(\"Shape del dataset:\", df.shape)\n",
    "    else:\n",
    "        print(\"Errore nel download del file\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dcdaeca-91fc-472a-8bbd-251e24019834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def initialize_stopwords(stop_words: set = None) -> set:\n",
    "    \"\"\"Initialize analyzer with optional custom stopwords\"\"\"\n",
    "    try:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        return stop_words or set(stopwords.words('english')).union({\n",
    "            'because', 'where', 'which', 'when',\n",
    "            'being', 'having', 'making', 'saying', 'every',\n",
    "            'everyone', 'everything', 'several'\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not download NLTK stopwords. Using basic stopwords. Error: {e}\")\n",
    "        return stop_words or {\n",
    "            'because', 'where', 'which', 'when',\n",
    "            'being', 'having', 'making', 'saying', 'every',\n",
    "            'everyone', 'everything', 'several'\n",
    "        }\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return str(text).lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def create_category_analysis(texts: pd.Series, title: str, stop_words: set) -> Tuple[Optional[plt.Figure], Optional[plt.Figure]]:\n",
    "    \"\"\"Create wordcloud and frequency histogram for a category\"\"\"\n",
    "    cleaned_text = ' '.join(clean_text(text) for text in texts if pd.notna(text))\n",
    "    \n",
    "    if not cleaned_text.strip():\n",
    "        print(f\"No valid text found for category {title}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Word frequency analysis\n",
    "    words = [w for w in cleaned_text.split() if w not in stop_words and len(w) >= 5]\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    try:\n",
    "        # Create wordcloud\n",
    "        wc = WordCloud(\n",
    "            width=1200, height=600,\n",
    "            background_color='white',\n",
    "            stopwords=stop_words,\n",
    "            max_words=100,\n",
    "            prefer_horizontal=0.7,\n",
    "            collocations=False,\n",
    "            random_state=42\n",
    "        ).generate(' '.join(words))\n",
    "        \n",
    "        # Create two figures\n",
    "        fig_cloud = plt.figure(figsize=(15, 8))\n",
    "        ax_cloud = fig_cloud.add_subplot(111)\n",
    "        ax_cloud.imshow(wc, interpolation='bilinear')\n",
    "        ax_cloud.axis('off')\n",
    "        ax_cloud.set_title(f'Word Cloud - {title}', size=16, pad=20)\n",
    "        \n",
    "        # Create histogram\n",
    "        fig_hist = plt.figure(figsize=(12, 6))\n",
    "        ax_hist = fig_hist.add_subplot(111)\n",
    "        words_freq = pd.DataFrame(word_freq.most_common(20), columns=['word', 'frequency'])\n",
    "        sns.barplot(data=words_freq, x='frequency', y='word', ax=ax_hist)\n",
    "        ax_hist.set_title(f'Top 20 Most Frequent Words - {title}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig_cloud, fig_hist\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating visualizations for {title}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def compute_basic_metrics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute basic metrics for analysis\"\"\"\n",
    "    df = df.copy()\n",
    "    df['word_count'] = df['documents'].str.split().str.len()\n",
    "    return df\n",
    "\n",
    "def get_basic_statistics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate basic statistics by category\"\"\"\n",
    "    stats = df.groupby('categoria').agg({\n",
    "        'categoria': 'count',  # numero di articoli\n",
    "        'word_count': ['mean', 'min', 'max']  # statistiche sulle parole\n",
    "    })\n",
    "    \n",
    "    # Rinomina le colonne per maggiore chiarezza\n",
    "    stats.columns = ['Numero articoli', 'Media parole', 'Minimo parole', 'Massimo parole']\n",
    "    return stats.round(2)\n",
    "\n",
    "def print_statistics(stats: pd.DataFrame):\n",
    "    \"\"\"Print statistics in a readable format\"\"\"\n",
    "    print(\"\\nANALISI STATISTICA PER CATEGORIA:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for categoria in stats.index:\n",
    "        print(f\"\\nCategoria: {categoria}\")\n",
    "        print(f\"  • Numero di articoli: {stats.loc[categoria, 'Numero articoli']}\")\n",
    "        print(f\"  • Media parole per articolo: {stats.loc[categoria, 'Media parole']:.1f}\")\n",
    "        print(f\"  • Articolo più corto: {stats.loc[categoria, 'Minimo parole']:.0f} parole\")\n",
    "        print(f\"  • Articolo più lungo: {stats.loc[categoria, 'Massimo parole']:.0f} parole\")\n",
    "\n",
    "def create_category_distribution(df: pd.DataFrame) -> plt.Figure:\n",
    "    \"\"\"Create distribution plot of articles by category\"\"\"\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    cat_counts = df['categoria'].value_counts()\n",
    "    sns.barplot(x=cat_counts.index, y=cat_counts.values, ax=ax)\n",
    "    ax.set_title('Distribuzione Articoli per Categoria')\n",
    "    ax.set_xlabel('Categoria')\n",
    "    ax.set_ylabel('Numero di Articoli')\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analisi_esplorativa(df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Main function to perform basic exploratory analysis\"\"\"\n",
    "    # Initialize stopwords\n",
    "    stop_words = initialize_stopwords()\n",
    "    \n",
    "    # Compute metrics\n",
    "    df = compute_basic_metrics(df)\n",
    "    \n",
    "    # Get and print basic statistics\n",
    "    statistics = get_basic_statistics(df)\n",
    "    print_statistics(statistics)\n",
    "    \n",
    "    # Create distribution plot\n",
    "    dist_plot = create_category_distribution(df)\n",
    "    \n",
    "    # Create wordclouds and histograms for each category\n",
    "    visualizations = {}\n",
    "    for categoria in df['categoria'].unique():\n",
    "        texts = df[df['categoria'] == categoria]['documents']\n",
    "        wordcloud, histogram = create_category_analysis(texts, categoria, stop_words)\n",
    "        visualizations[categoria] = {\n",
    "            'wordcloud': wordcloud,\n",
    "            'histogram': histogram\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'visualizations': visualizations,\n",
    "        'statistics': statistics,\n",
    "        'distribution': dist_plot\n",
    "    }\n",
    "\n",
    "# Usage\n",
    "risultati = analisi_esplorativa(df)\n",
    "plt.show()  # To display all plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d48fd1-471b-4432-9d8e-0e6d74760fc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fine analisi esplorativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8b13629-2922-40bf-8829-1537109b1826",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# DataFrame da Pandas a Spark \n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "def prepara_dati(spark_df):\n",
    "    \"\"\"\n",
    "    Preparazione dei dati per il modello di classificazione usando PySpark\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Preparazione dei Dati ===\")\n",
    "    \n",
    "    # Verifichiamo i valori nulli\n",
    "    print(\"\\nValori nulli nel dataset:\")\n",
    "    null_counts = spark_df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) for c in spark_df.columns])\n",
    "    null_counts.show()\n",
    "    \n",
    "    # Pulizia dei dati\n",
    "    df_clean = spark_df.dropna(subset=['documents', 'categoria'])\n",
    "    \n",
    "    rows_before = spark_df.count()\n",
    "    rows_after = df_clean.count()\n",
    "    print(f\"\\nRighe rimosse per valori nulli: {rows_before - rows_after}\")\n",
    "    \n",
    "    # Preparazione delle features\n",
    "    # Pipeline di preprocessing del testo\n",
    "    tokenizer = Tokenizer(inputCol=\"documents\", outputCol=\"words\")\n",
    "    hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\", numFeatures=5000)\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\", minDocFreq=2)\n",
    "    \n",
    "    # Indicizzazione delle categorie\n",
    "    indexer = StringIndexer(inputCol=\"categoria\", outputCol=\"label\")\n",
    "    \n",
    "    # Pipeline\n",
    "    pipeline = Pipeline(stages=[\n",
    "        tokenizer,\n",
    "        hashingTF,\n",
    "        idf,\n",
    "        indexer\n",
    "    ])\n",
    "    \n",
    "    # Split dei dati\n",
    "    train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    print(\"\\nDimensioni dei set di dati prima della trasformazione:\")\n",
    "    print(f\"Training set: {train_df.count()} righe\")\n",
    "    print(f\"Test set: {test_df.count()} righe\")\n",
    "    \n",
    "    # Fit della pipeline\n",
    "    print(\"\\nApplico la pipeline di trasformazione...\")\n",
    "    pipeline_model = pipeline.fit(train_df)\n",
    "    \n",
    "    # Transform dei dati\n",
    "    train_transformed = pipeline_model.transform(train_df)\n",
    "    test_transformed = pipeline_model.transform(test_df)\n",
    "    \n",
    "    # Selezione delle sole colonne necessarie per il training\n",
    "    final_columns = [\"label\", \"features\"]\n",
    "    train_final = train_transformed.select(final_columns)\n",
    "    test_final = test_transformed.select(final_columns)\n",
    "    \n",
    "    print(\"\\nDimensioni finali dei set di dati:\")\n",
    "    print(f\"Training set: {train_final.count()} righe\")\n",
    "    print(f\"Test set: {test_final.count()} righe\")\n",
    "    \n",
    "    return train_final, test_final, pipeline_model\n",
    "\n",
    "# prima in Spark DataFrame e poi preparazione dei dati\n",
    "spark_df = spark.createDataFrame(df)\n",
    "train_df, test_df, pipeline_model = prepara_dati(spark_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e71b2e24-69f8-4591-a077-2435ba645f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, HashingTF, IDF, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "def crea_modello_classificazione(spark_df):\n",
    "    \"\"\"\n",
    "    Creazione e valutazione di un modello di classificazione usando sia sommario che testo completo\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Creazione Modello di Classificazione ===\")\n",
    "    \n",
    "    # Verifica dati\n",
    "    print(\"\\nValori nulli nel dataset:\")\n",
    "    null_counts = spark_df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c) \n",
    "                                 for c in ['summary', 'documents', 'categoria']])\n",
    "    null_counts.show()\n",
    "    \n",
    "    # Pulizia dati\n",
    "    df_clean = spark_df.dropna(subset=['summary', 'documents', 'categoria'])\n",
    "    \n",
    "    rows_before = spark_df.count()\n",
    "    rows_after = df_clean.count()\n",
    "    print(f\"\\nRighe rimosse per valori nulli: {rows_before - rows_after}\")\n",
    "    \n",
    "    # Pipeline di preprocessing\n",
    "    # sommario\n",
    "    summary_tokenizer = Tokenizer(inputCol=\"summary\", outputCol=\"summary_words\")\n",
    "    summary_hashingTF = HashingTF(inputCol=\"summary_words\", outputCol=\"summary_raw_features\", numFeatures=2000)\n",
    "    summary_idf = IDF(inputCol=\"summary_raw_features\", outputCol=\"summary_features\", minDocFreq=2)\n",
    "    \n",
    "    # testo completo\n",
    "    doc_tokenizer = Tokenizer(inputCol=\"documents\", outputCol=\"doc_words\")\n",
    "    doc_hashingTF = HashingTF(inputCol=\"doc_words\", outputCol=\"doc_raw_features\", numFeatures=5000)\n",
    "    doc_idf = IDF(inputCol=\"doc_raw_features\", outputCol=\"doc_features\", minDocFreq=2)\n",
    "    \n",
    "    # Combina le features\n",
    "    assembler = VectorAssembler(inputCols=[\"summary_features\", \"doc_features\"], outputCol=\"features\")\n",
    "    \n",
    "    # Indicizzazione delle categorie\n",
    "    indexer = StringIndexer(inputCol=\"categoria\", outputCol=\"label\")\n",
    "    \n",
    "    # Modello di classificazione\n",
    "    lr = LogisticRegression(maxIter=20, elasticNetParam=0.5)\n",
    "    \n",
    "    # Pipeline completa\n",
    "    pipeline = Pipeline(stages=[\n",
    "        summary_tokenizer,\n",
    "        summary_hashingTF,\n",
    "        summary_idf,\n",
    "        doc_tokenizer,\n",
    "        doc_hashingTF,\n",
    "        doc_idf,\n",
    "        assembler,\n",
    "        indexer,\n",
    "        lr\n",
    "    ])\n",
    "    \n",
    "    # Split dei dati\n",
    "    train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    print(\"\\nDimensioni dei set di dati:\")\n",
    "    print(f\"Training set: {train_df.count()} righe\")\n",
    "    print(f\"Test set: {test_df.count()} righe\")\n",
    "    \n",
    "    # Training del modello\n",
    "    print(\"\\nAddestramento del modello...\")\n",
    "    model = pipeline.fit(train_df)\n",
    "    \n",
    "    # Valutazione sul test set\n",
    "    print(\"\\nValutazione del modello...\")\n",
    "    predictions = model.transform(test_df)\n",
    "    \n",
    "    # Calcolo metriche\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    \n",
    "    # Calcolo accuracy\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    print(f\"\\nAccuracy sul test set: {accuracy:.4f}\")\n",
    "    \n",
    "    # Calcolo precision e recall per ogni categoria\n",
    "    print(\"\\nMetriche per categoria:\")\n",
    "    label_dict = {float(idx): cat for idx, cat in enumerate(model.stages[-2].labels)}\n",
    "    \n",
    "    # Calcolo della confusion matrix\n",
    "    conf_matrix = (predictions.groupBy(\"label\", \"prediction\").count().toPandas())\n",
    "    \n",
    "    # Calcolo metriche per classe\n",
    "    for label in label_dict:\n",
    "        true_pos = conf_matrix[\n",
    "            (conf_matrix.label == label) & \n",
    "            (conf_matrix.prediction == label)]['count'].sum()\n",
    "        \n",
    "        false_pos = conf_matrix[\n",
    "            (conf_matrix.label != label) & \n",
    "            (conf_matrix.prediction == label)]['count'].sum()\n",
    "        \n",
    "        false_neg = conf_matrix[\n",
    "            (conf_matrix.label == label) & \n",
    "            (conf_matrix.prediction != label)]['count'].sum()\n",
    "        \n",
    "        precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "        recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"\\nCategoria: {label_dict[label]}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    return model, accuracy, predictions\n",
    "\n",
    "# Esecuzione\n",
    "spark_df = spark.createDataFrame(df)\n",
    "model, accuracy, predictions = crea_modello_classificazione(spark_df)\n",
    "\n",
    "# Per predizioni su nuovi dati\n",
    "def predici_categoria(model, testo, sommario):\n",
    "    \"\"\"\n",
    "    Predice la categoria di un nuovo articolo\n",
    "    \"\"\"\n",
    "    # Creare un DataFrame con il nuovo testo\n",
    "    nuovo_df = spark.createDataFrame(\n",
    "        [(sommario, testo)],\n",
    "        [\"summary\", \"documents\"]\n",
    "    )\n",
    "    \n",
    "    # Applicare il modello\n",
    "    prediction = model.transform(nuovo_df)\n",
    "    \n",
    "    # Ottenere la categoria predetta\n",
    "    categoria_idx = prediction.select(\"prediction\").first()[0]\n",
    "    categoria = model.stages[-2].labels[int(categoria_idx)]\n",
    "    \n",
    "    return categoria\n",
    "\n",
    "# categoria_predetta = predici_categoria(model, \"testo\", \"sommario\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc93079c-f0c9-4bd7-8d7b-0dbf2258243d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Il modello di classificazione automatica degli articoli Wikimedia ha raggiunto risultati eccellenti, con un'accuratezza superiore al 91%. La quasi totalità delle categorie tematiche viene classificata correttamente con F1-Score superiori al 90%, ad eccezione di \"medicine\" e \"research\" che si attestano intorno al 75%. Questo suggerisce che il modello è affidabile e pronto per essere utilizzato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "226a9784-697c-442b-ab45-3fba20e97616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def analisi_contenuti_wikimedia(df):\n",
    "    \"\"\"\n",
    "    Analisi dei contenuti Wikimedia:\n",
    "    - Densità articoli per categoria\n",
    "    - Parole più frequenti per categoria\n",
    "    \"\"\"\n",
    "    print(\"\\n=== ANALISI CONTENUTI WIKIMEDIA ===\")\n",
    "    \n",
    "    # 1. Densità articoli per categoria\n",
    "    print(\"\\n1. Densità degli articoli\")\n",
    "    \n",
    "    densita = df['categoria'].value_counts()\n",
    "    percentuali = df['categoria'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(\"\\nDistribuzione degli articoli per categoria:\")\n",
    "    for cat in densita.index:\n",
    "        print(f\"  • {cat}: {densita[cat]} articoli ({percentuali[cat]:.1f}%)\")\n",
    "    \n",
    "    # Visualizzazione della densità\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=densita.values, y=densita.index)\n",
    "    plt.title('Densità degli articoli per categoria')\n",
    "    plt.xlabel('Numero di articoli')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 2. Analisi parole frequenti\n",
    "    # Parole da escludere\n",
    "    stop_words = {\n",
    "        'the', 'and', 'for', 'that', 'was', 'with', 'this', 'from', 'his', 'her', \n",
    "        'they', 'are', 'were', 'their', 'one', 'all', 'had', 'but', 'not', 'what',\n",
    "        'when', 'who', 'which', 'she', 'would', 'been', 'will', 'there', 'more',\n",
    "        'has', 'link', 'links', 'external', 'reference', 'references', 'http', 'https',\n",
    "        'www', 'com', 'org', 'ref', 'cite', 'cited', 'page', 'pages', 'retrieved',\n",
    "        'volume', 'edition', 'press', 'published'\n",
    "    }\n",
    "    \n",
    "    def pulisci_e_conta_parole(testi):\n",
    "        \"\"\"Pulisce e conta le parole più frequenti (max una per articolo)\"\"\"\n",
    "        try:\n",
    "            # Conteggio in quanti articoli appare ogni parola\n",
    "            word_in_articles = Counter()\n",
    "            \n",
    "            for testo in testi.dropna():\n",
    "                # Convertire in minuscolo\n",
    "                testo = testo.lower()\n",
    "                \n",
    "                # Rimuovere caratteri speciali e numeri\n",
    "                testo_pulito = re.sub(r'[^\\w\\s]', ' ', testo)\n",
    "                testo_pulito = re.sub(r'\\d+', ' ', testo_pulito)\n",
    "                \n",
    "                # Tokenizzazione\n",
    "                words = word_tokenize(testo_pulito)\n",
    "                \n",
    "                # Creare set di parole uniche per questo articolo\n",
    "                article_words = {\n",
    "                    w for w in words \n",
    "                    if len(w) >= 5  # esclude parole troppo corte\n",
    "                    and w not in stop_words  # esclude stop words\n",
    "                    and not any(c.isdigit() for c in w)  # esclude parole con numeri\n",
    "                }\n",
    "                \n",
    "                # Aggiornare il conteggio\n",
    "                word_in_articles.update(article_words)\n",
    "            \n",
    "            return word_in_articles.most_common(15)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Errore nel conteggio parole: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    print(\"\\n2. Tendenze linguistiche per categoria\")\n",
    "    \n",
    "    # Analisi per ogni categoria\n",
    "    for categoria in df['categoria'].unique():\n",
    "        testi = df[df['categoria'] == categoria]['documents']\n",
    "        n_articoli = len(testi)\n",
    "        parole_freq = pulisci_e_conta_parole(testi)\n",
    "        \n",
    "        if parole_freq:\n",
    "            print(f\"\\nParole più frequenti in {categoria} ({n_articoli} articoli):\")\n",
    "            for parola, freq in parole_freq:\n",
    "                perc = (freq / n_articoli) * 100\n",
    "                print(f\"  • {parola:<15} {freq:>6} occorrenze ({perc:>5.1f}% degli articoli)\")\n",
    "    \n",
    "    return {\n",
    "        'densita': densita,\n",
    "        'percentuali': percentuali\n",
    "    }\n",
    "\n",
    "# Esecuzione dell'analisi\n",
    "risultati = analisi_contenuti_wikimedia(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cea64d98-f859-40a6-b55e-a3fa1139e490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "L'analisi mostra una distribuzione quasi uniforme degli articoli tra le categorie, con una leggera predominanza della categoria \"politics\". Per quanto riguarda le tendenze linguistiche, ogni categoria mostra parole chiave fortemente caratterizzanti: ad esempio \"power\" e \"plant\" sono presenti in oltre il 60% degli articoli di energia, \"tennis\" domina la categoria sport, e \"university\" e \"research\" sono molto frequenti nella categoria ricerca (oltre l'80% degli articoli)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1787414503914456,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
